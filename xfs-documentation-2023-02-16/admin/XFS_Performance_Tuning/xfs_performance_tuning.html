<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>XFS Performance Tuning</title><link rel="stylesheet" type="text/css" href="docbook-xsl.css" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><meta name="description" content="This book documents the knowledge and processes required to optimise the XFS filesystem." /></head><body><div xml:lang="en" class="book" lang="en"><div class="titlepage"><div><div><h1 class="title"><a id="idm1"></a>XFS Performance Tuning</h1></div><div><h2 class="subtitle">Performance Optimisation of the XFS Filesystem</h2></div><div><h3 class="corpauthor">
</h3></div><div><p class="copyright">Copyright © 2014 Red Hat, Inc.</p></div><div><div class="legalnotice"><a id="idm11"></a><p>© Copyright 2014 Red Hat, Inc. All rights reserved.</p><p>Permission is granted to copy, distribute, and/or modify this
        document under the terms of the Creative Commons Attribution-Share
        Alike, Version 3.0 Unported License. A copy of the license is available
        at <a class="ulink" href="http://creativecommons.org/licenses/by-sa/3.0/us/" target="_top">http://creativecommons.org/licenses/by-sa/3.0/us/</a>.</p></div></div><div><div class="revhistory"><table style="border-style:solid; width:100%;" summary="Revision History"><tr><th align="left" valign="top" colspan="3"><strong>Revision History</strong></th></tr><tr><td align="left">Revision 0.1</td><td align="left">2014</td><td align="left"><span class="firstname">Dave<br /></span><span class="surname">Chinner<br /></span><code class="email">&lt;<a class="email" href="mailto:dchinner@redhat.com">dchinner@redhat.com</a>&gt;</code></td></tr><tr><td align="left" colspan="3">
                        <table border="0" summary="Simple list" class="simplelist"><tr><td>Initial Version</td></tr></table>
                </td></tr></table></div></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><p>This book documents the knowledge and processes required to
        optimise the XFS filesystem.</p></div></div></div><hr /></div><div class="toc"><p><strong>Table of Contents</strong></p><dl class="toc"><dt><span class="preface"><a href="#idm26"></a></span></dt><dt><span class="part"><a href="#_introduction">I. Introduction</a></span></dt><dd><dl><dt><span class="chapter"><a href="#_overview">1. Overview</a></span></dt></dl></dd><dt><span class="part"><a href="#Knowledge">II. Knowledge</a></span></dt><dd><dl><dt><span class="chapter"><a href="#Filesystem_Tunables">2. Filesystem Tunables</a></span></dt><dd><dl><dt><span class="section"><a href="#_formatting_options">2.1. Formatting options</a></span></dt><dt><span class="section"><a href="#_mount_options">2.2. Mount options</a></span></dt></dl></dd></dl></dd><dt><span class="part"><a href="#Observation">III. Observation</a></span></dt><dd><dl><dt><span class="chapter"><a href="#_todo">3. TODO</a></span></dt></dl></dd><dt><span class="part"><a href="#Process">IV. Tuning Processes</a></span></dt><dd><dl><dt><span class="chapter"><a href="#_todo_2">4. TODO</a></span></dt></dl></dd></dl></div><div class="list-of-tables"><p><strong>List of Tables</strong></p><dl><dt>2.1. <a href="#idm142">Average directory read and inode look up rate when CPU bound</a></dt><dt>2.2. <a href="#idm202">Average directory read and inode look up rate when IO bound</a></dt><dt>2.3. <a href="#idm263">Average file remove rate when CPU bound</a></dt><dt>2.4. <a href="#idm323">Average file remove rate when IO bound</a></dt><dt>2.5. <a href="#Dirblocksize_table">Recommended maximum number of directory entries for directory block sizes</a></dt></dl></div><div class="preface"><div class="titlepage"><div><div><h1 class="title"><a id="idm26"></a></h1></div></div></div><p><span class="emphasis"><em>Premature optimization is the root of all evil.</em></span> - Donald Knuth</p><p>So you want to tune your XFS filesystem for a workload? Think hard about it as
it’s difficult to do properly and requires significant knowledge and expertise.
Reading this guide won’t make you an optimisation expert, though it might just
help you solve the problem you are facing.</p></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a id="_introduction"></a>Part I. Introduction</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="_overview"></a>Chapter 1. Overview</h2></div></div></div><p>This document describes the theory and practice being tuning XFS
for performance. Performance tuning is a mixture of knowledge, process and
observation. The <a class="link" href="#Knowledge" title="Part II. Knowledge">Knowledge Section</a> will cover aspects of the XFS
configuration space that affect performance and assumes that the reader has some
familiarity with the XFS on-disk structures.</p><p>The <a class="link" href="#Observation" title="Part III. Observation">Observation Section</a> will demonstrate various tools that
can be used to monitor filesystem performance while workloads are run. Key
performance metrics will be discussed and related back to the structure of the
filesystem and configuration parameters discussed in the
<a class="link" href="#Knowledge" title="Part II. Knowledge">Knowledge Section</a>.</p><p>The <a class="link" href="#Process" title="Part IV. Tuning Processes">Process section</a> will cover the typical processes used to
optimise a filesystem for a given workload. If the workload measurements are not
accurate or reproducible, then no conclusions can be drawn as to whether a
configuration changes an improvement or not. Hence without a robust testing
process, no amount of knowledge or observation will result in a well optimised
filesystem configuration.</p><p>While reading this document, it is important to keep in mind the fact that
there are relatively few workloads where using non-default mkfs.xfs or mount
options make much sense. In general, the default values already used are
optimised for best performance in the first place. The most common
optimisations are done automatically if possible and hence mostly remove the
need for manual optimisation.</p><p>Therefore, you should only consider changing the defaults if either:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
you know from experience that your workload causes XFS a specific problem that
can be worked around via a configuration change, or
</li><li class="listitem">
your workload is demonstrating bad performance when using the default
configurations.
</li></ul></div><p>In this case, you need to understand why your application is causing bad
performance before you start tweaking XFS configurations. This guide is intended
to help you identify the cause of perofrmance problems and which knobs you can
tweak to avoid or mitigate the problems being observed.</p></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a id="Knowledge"></a>Part II. Knowledge</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="Filesystem_Tunables"></a>Chapter 2. Filesystem Tunables</h2></div></div></div><p>This section covers some of the tuning parameters available to XFS file systems
at format and at mount time that are relevant to performance optimisation.</p><p>The default formatting and mount settings for XFS are suitable for most
workloads. Specific tunings should only be considered if either:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem">
it is known from experience that the  workload causes XFS a specific problem
that can be worked around via a configuration change, or
</li><li class="listitem">
a workload is demonstrating bad performance when using the default
configurations.
</li></ul></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_formatting_options"></a>2.1. Formatting options</h2></div></div></div><p>For further details about the syntax and usage any of these formatting options,
see the man page:</p><pre class="screen">$ man mkfs.xfs</pre><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>Please pay particular attention to the different units used by the different
<code class="literal">mkfs.xfs</code> commands. Also, <code class="literal">mkfs.xfs</code> and <code class="literal">xfs_info</code> report the selected
configuration in units of filesystem blocks and these are not necessarily same
units as used on the <code class="literal">mkfs.xfs</code> command line to configure these parameters.</p></div><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>The performance examples given in this section are highly dependent on storage,
CPU and RAM configuration. They are intended as guidelines to illustrate
behavioural differences, not the exact performance any configuration will
achieve.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_directory_block_size"></a>2.1.1. Directory block size</h3></div></div></div><p>The directory block size affects the amount of directory information that can be
retrieved or modified per I/O operation. The minimum value for directory block
size is the file system block size (4 KB by default) and the maximum directory
block size is 64KB.</p><p>Directory block size can only be configured at mkfs time by using the following
option:</p><pre class="screen">mkfs.xfs -n size=&lt;num&gt;</pre><p>There are various trade-offs that have to be considered when selecting an
optimum directory block size. Because directories in XFS are btree structures,
performance decrease due to IO overhead is logarithmic with respect to
increasing directory size. However, CPU overhead per modification operation
increases linearly with directory block size, and hence large directory block
sizes will only out-perform small directory block sizes once the IO overhead of
manipulating the small directory is greater than the CPU overhead of large
directory blocks.</p><p>An artificial workload that demonstrated this CPU/IO trade off is creating zero
length files in a directory. If the directory is small, the workload will be CPU
bound, but if the directory is large it will become IO bound and so the
performance profile will change.</p><div class="informaltable"><table class="informaltable" cellpadding="4px" style="border-collapse: collapse;border-top: 3px solid #527bbd; border-bottom: 3px solid #527bbd; border-left: 3px solid #527bbd; border-right: 3px solid #527bbd; "><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><tbody><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Directory block size</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>10000 entries</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100K entries</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1M entries</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>21,500/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>15,500/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>6,700/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>16k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>18,500/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>14,400/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>11,000/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>64k</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>14,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>9,500/s</strong></span></p></td><td style="" align="left" valign="top"><p><span class="strong"><strong>7,500/s</strong></span></p></td></tr></tbody></table></div><p>It can be seen from the table that 4k directory block size has the largest
modification rate decay as the directory size increases and IO overhead starts
to dominate. 64k block sizes have the lowest decay rate because that
configuration has the lowest IO overhead and it won’t be until the directories
contain more than 10 million entries that significant performance degradation
will be observed.</p><p>Read dominated workloads are more difficult to characterise because best
performance comes from keeping a working set of directories and files cached in
RAM. Ignoring this aspect of the performance tuning problem, look up performance
of directories is really an IO bound workload. hence the fewer IOs that need to
be done to find a directory entry, the better the performance will be.</p><p>Again, larger directory blocks will consume more CPU per look up than smaller
directory blocks, but larger directories blocks can index a far greater number
of entries per IO and so the CPU overhead of searching a large block is less
than doing an extra IO. Hence a look up dominated workload will have a much lower
cross-over point where larger block sizes will perform better.</p><div class="table"><a id="idm142"></a><p class="title"><strong>Table 2.1. Average directory read and inode look up rate when CPU bound</strong></p><div class="table-contents"><table class="table" summary="Average directory read and inode look up rate when CPU bound" cellpadding="4px" style="border-collapse: collapse;border-top: 3px solid #527bbd; border-bottom: 3px solid #527bbd; border-left: 3px solid #527bbd; border-right: 3px solid #527bbd; "><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><tbody><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Directory block size</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>10000 entries</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100K entries</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1M entries</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>32,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>25,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>18,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>16k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>42,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>26,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>22,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>64k</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>42,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>26,000/s</strong></span></p></td><td style="" align="left" valign="top"><p><span class="strong"><strong>22,000/s</strong></span></p></td></tr></tbody></table></div></div><br class="table-break" /><div class="table"><a id="idm202"></a><p class="title"><strong>Table 2.2. Average directory read and inode look up rate when IO bound</strong></p><div class="table-contents"><table class="table" summary="Average directory read and inode look up rate when IO bound" cellpadding="4px" style="border-collapse: collapse;border-top: 3px solid #527bbd; border-bottom: 3px solid #527bbd; border-left: 3px solid #527bbd; border-right: 3px solid #527bbd; "><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><tbody><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Directory block size</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>10000 entries</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100K entries</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1M entries</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>8,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>7,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>6,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>16k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>32,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>9,500/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>8,000/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>64k</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>32,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>10,000/s</strong></span></p></td><td style="" align="left" valign="top"><p><span class="strong"><strong>11,000/s</strong></span></p></td></tr></tbody></table></div></div><br class="table-break" /><p>And, finally, removing those files is a combination of look up and modification
overhead, so it can be either IO or CPU bound depending on which part of the
remove operation is the limiting factor:</p><div class="table"><a id="idm263"></a><p class="title"><strong>Table 2.3. Average file remove rate when CPU bound</strong></p><div class="table-contents"><table class="table" summary="Average file remove rate when CPU bound" cellpadding="4px" style="border-collapse: collapse;border-top: 3px solid #527bbd; border-bottom: 3px solid #527bbd; border-left: 3px solid #527bbd; border-right: 3px solid #527bbd; "><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><tbody><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Directory block size</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>10000 entries</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100K entries</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1M entries</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>11,400/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>18,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>3,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>16k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>11,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>17,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>9,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>64k</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>9,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>14,000/s</strong></span></p></td><td style="" align="left" valign="top"><p><span class="strong"><strong>7,000/s</strong></span></p></td></tr></tbody></table></div></div><br class="table-break" /><div class="table"><a id="idm323"></a><p class="title"><strong>Table 2.4. Average file remove rate when IO bound</strong></p><div class="table-contents"><table class="table" summary="Average file remove rate when IO bound" cellpadding="4px" style="border-collapse: collapse;border-top: 3px solid #527bbd; border-bottom: 3px solid #527bbd; border-left: 3px solid #527bbd; border-right: 3px solid #527bbd; "><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /><col class="col_4" /></colgroup><tbody><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Directory block size</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>10000 entries</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100K entries</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1M entries</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4,500/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>2,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>16k</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>5,500/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>7,000/s</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4,500/s</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>64k</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>6,000/s</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>6,500/s</strong></span></p></td><td style="" align="left" valign="top"><p><span class="strong"><strong>5,000/s</strong></span></p></td></tr></tbody></table></div></div><br class="table-break" /><p>Typical directory block sizes configurations for different workloads and
directory sizes are listed in <a class="xref" href="#Dirblocksize_table" title="Table 2.5. Recommended maximum number of directory entries for directory block sizes">Table 2.5, “Recommended maximum number of directory entries for directory block sizes”</a>. These are only
guidelines - the best size for any given workload will vary. When in doubt, use
the mkfs default values.</p><div class="table"><a id="Dirblocksize_table"></a><p class="title"><strong>Table 2.5. Recommended maximum number of directory entries for directory block sizes</strong></p><div class="table-contents"><table class="table" summary="Recommended maximum number of directory entries for directory block sizes" cellpadding="4px" style="border-collapse: collapse;border-top: 3px solid #527bbd; border-bottom: 3px solid #527bbd; border-left: 3px solid #527bbd; border-right: 3px solid #527bbd; "><colgroup><col class="col_1" /><col class="col_2" /><col class="col_3" /></colgroup><tbody><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Directory block size</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Max. entries (read-heavy)</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>Max. entries (write-heavy)</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>4 KB</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100000-200000</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1000000-2000000</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>16 KB</strong></span></p></td><td style="border-right: 1px solid #527bbd; border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>100000-1000000</strong></span></p></td><td style="border-bottom: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>1000000-10000000</strong></span></p></td></tr><tr><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>64 KB</strong></span></p></td><td style="border-right: 1px solid #527bbd; " align="left" valign="top"><p><span class="strong"><strong>&gt;1000000</strong></span></p></td><td style="" align="left" valign="top"><p><span class="strong"><strong>&gt;10000000</strong></span></p></td></tr></tbody></table></div></div><br class="table-break" /></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_allocation_groups"></a>2.1.2. Allocation groups</h3></div></div></div><p>An Allocation Group (AG) in XFS in an independent structure that indexes free
space and allocated inodes across a section of the filesystem. Each AG can be
modified independently and hence allows XFS to be able to perform concurrent
space management operations. Space management concurrency is limited by the
number of AGs in the filesystem. This concurrency limitation will be seen by
workloads that heavily stress the space management routines. Contrary to commen
expectations, data intensive workloads don’t tend to be space management
concurrency limited - concurrent directory and attribute modification workloads
tend to stress space management concurrency far more. Hence tuning a filesystem
for the correct number of AGs is not as straight forward as it may seem.</p><p>The <code class="literal">mkfs.xfs</code> default configuration is dependent on the configuration of the
underlying storage. The underlying storage has the capability to sustain a
certain amount IO concurrency and feeding that requires a certain amount of
allocation conncurency. Filesystems on a single spindle have extremely limited
IO concurrency capability, while a RAID array has far more. As such, <code class="literal">mkfs.xfs</code>
will default to 4 AGs for single spindle storage and default to 32 AGs for
RAID based storage.</p><p>For most workloads, the defaults will be sufficient. In general, the number of
AGs only needs tuning if <code class="literal">mkfs.xfs</code> cannot detect the type of storage underlying
the filesystem or a highly concurrent workload is being used on a filesystem of
limited size. In these cases, ensuring that there are at least 32 AGs in the
filesystem will ensure that XFS can extract all the concurrency available in
the underlying storage.</p><p>The number of allocation groups can only be configured at mkfs time by using the
following option:</p><pre class="screen">mkfs.xfs -d agcount=&lt;num&gt;</pre><p>See the <code class="literal">mkfs.xfs</code> man page for details.</p><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_alignment_to_storage_geometry"></a>Alignment to storage geometry</h4></div></div></div><p>TODO: This is extremely complex and requires an entire chapter to itself.</p></div><div class="section"><div class="titlepage"><div><div><h4 class="title"><a id="_constraints_for_growing_filesystems"></a>Constraints for Growing Filesystems</h4></div></div></div><p>When a storage system is configured to be grown in future (either by adding more
hardware or thin provisioning) special consideration needs to be given to the
initial filesystem layout. The size of the allocation group cannot be changed
after the initial <code class="literal">mkfs.xfs</code> execution so this constrains how the filesystem can
be grown.</p><p>A key concern is that the allocation groups need to be sized according to the
eventual capacity of the filesystem, not the initial capacity. Unless the
size of the AGs are at their maximum (1TB), the number of AGs in the fully grown
filesystem should not exceed more than a few hundred. This means that for
typical filesystems, a size growth of roughly 10x over the initial provisioning
if the maximum recommended.</p><p>Other considerings include the underlying geometry of the storage. If you are
going to grow a filesystem on a RAID array, care needs to be taken to align the
device size to an exact multiple of the AG size. This means that the last AG in
the filesystem does not extend onto the new space when it is grown, and so that
new AG headers are correctly aligned on the newly added storage. It is also
important that new storage has the same geometry as the existing storage, as the
filesystem geometry cannot be changed and so cannot be optimised for storage of
different geometries in the one block device.</p></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_inode_size_and_inline_attributes"></a>2.1.3. Inode size and inline attributes</h3></div></div></div><p>If the inode has sufficient space available, XFS can write attribute names and
values directly into the inode. These inline attributes can be retrieved and
modified up to an order of magnitude faster than retrieving separate attribute
blocks, as additional I/O is not required.</p><p>The default inode size is dependent on whether metadata CRCs are enabled or not.
CRCs are not enabled by default; the default inode size for this configuration
is 256 bytes. When CRCs are enabled, the minimum supported inode size is
increased to 512 bytes and this is used as the default.</p><p>Inline attributes are stored in the literal area of the inode. The size of this
region is dynamic and is shared between data and attribute content. The maximum
space available to attributes is determined by the size of the inode core and
the the number of data extent pointers stored in the inode. For the default
inode sizes, there is a maximum of around 100 bytes for default mkfs
configurations and around 300 bytes for CRC enabled filesystems. Increasing
inode size when you format the file system can increase the amount of space
available for storing attributes.</p><p>When attributes are stored in the literal area of the inode, both attribute
names and attribute values are limited to a maximum size of 254 bytes. If either
name or value exceeds 254 bytes in length, or the total space used by the
attributes exceeds the size of the literal area, the entire set of attributes
stored on the inode are pushed to a separate attribute block instead of being
stored inline.</p><p>The inode size  can only be configured at mkfs time by using the following
option:</p><pre class="screen">mkfs.xfs -i size=&lt;num&gt;</pre><p>See the <code class="literal">mkfs.xfs</code> man page for details.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_raid"></a>2.1.4. RAID</h3></div></div></div><p>If software RAID is in use, <code class="literal">mkfs.xfs</code> automatically configures itself with an
appropriate stripe unit and width for the underlying hardware. Hardware raid may
or may not expose the information in the block device to allow <code class="literal">mkfs.xfs</code> to
automatically configure them. Hence stripe unit and width may need to be
manually configured if hardware RAID is in use.</p><p>To configure stripe unit and width, use one of the following two options:</p><pre class="screen">mkfs.xfs -d sunit=&lt;num&gt;b,swidth=&lt;num&gt;b
mkfs.xfs -d su=&lt;size&gt;,sw=&lt;count&gt;</pre><p>See the <code class="literal">mkfs.xfs</code> man page for details.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_log_size"></a>2.1.5. Log size</h3></div></div></div><p>Pending changes are written to the log prior to being written to disk; they are
aggregated in memory until a synchronisation event is triggered. When this
happens, the aggregated changes are written to the journal. The size of the
journal determines maximum amount of change that can be aggregated in memory, as
well as the number of concurrent modifications that can be in flight at once.</p><p>Therefore, the size of the log determines the concurrency of metadata
modification operations the filesystem can sustain, as well as how much and how
frequently metadata writeback occurs.  A smaller log forces data
write-back more frequently than a larger log, but can result in lower
synchronisation overhead as there will be fewer changes aggreagted in memory
between synchronisation triggers. Memory pressure also generates synchronisatin
triggers, so large logs may not benefit systems with limited memory.</p><p>To configure the log size, use one the following <code class="literal">mkfs.xfs</code> option:</p><pre class="screen">mkfs.xfs -l size=&lt;num&gt;</pre><p>See the <code class="literal">mkfs.xfs</code> man page for details.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_log_stripe_unit"></a>2.1.6. Log stripe unit</h3></div></div></div><p>On storage devices using layouts such as RAID5 or RAID6, the log writes may
perform better when they are aligned to the underlying stripe unit. That
is, they start and end at stripe unit boundaries. When <code class="literal">mkfs.xfs</code> detects that the
underlying storage is a RAID device, it will attempt to set the log stripe unit
automatically.</p><p>If the workload being optimised triggers log synchronisation events frequently
(e.g. fsync() occurs very often), then setting a log stripe unit may reduce
performance even though the underlying device is a RAID device. This is caused
by the log writes needing to be padded to the log stripe unit size and this
causes an increase in log write latency compared to just writing a sector
aligned log buffer. Hence for log write latency bound workloads, setting the log
stripe unit to 1 block to trigger unaligned log writes as quickly as possible
may be optimal.</p><p>To configure the log stripe unit, use one of the following two options:</p><pre class="screen">mkfs.xfs -l sunit=&lt;num&gt;b
mkfs.xfs -l su=&lt;size&gt;</pre><p>See the <code class="literal">mkfs.xfs</code> man page for details.</p><div class="note" style="margin-left: 0; margin-right: 10%;"><h3 class="title">Note</h3><p>The maximum log stripe unit supported is limited to the maximum log buffer size
of 256KB, so the underlying storage may have a larger stripe unit than the log
can be configured with. When this happens, <code class="literal">mkfs.xfs</code> will issue a warning and use
32KB as the default.</p></div></div></div><div class="section"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a id="_mount_options"></a>2.2. Mount options</h2></div></div></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_inode_allocation"></a>2.2.1. Inode allocation</h3></div></div></div><p>XFS has two different strategies for inode and data allocation for filesystems
that are larger than 1 TB. The default allocation policy is <code class="literal">inode64</code>. This
parameter configures XFS to allocate inodes and data across the entire file
system whilst maintaining locality between inodes their related data.</p><p>The <code class="literal">inode32</code> option allocates inodes entirely within the first 1TB of
filesystem space so that the inode numbers do not exceed 32 bits in size. The
related file data is spread evenly across the rest of the filesystem, so there
is no locality between related inodes and their data. In most cases, this
configuration results is lower performance when compared to the <code class="literal">inode64</code>
configuration, but may be necessary for 32 bit applications to function
correctly. This is typically only a problem on 32 bit systems or on NFS exported
filesystems that are mounted by 32 bit NFS clients.</p></div><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="_log_buffer_size_and_count"></a>2.2.2. Log buffer size and count</h3></div></div></div><p>The larger the log buffer, the fewer IOs that are required to write the
aggregated changes in memory to disk. For storage subsystems that contain
non-volaile write caches, this will make little difference to performance, but
for other types of storage this reduction in log IOs should help improve
performance in IO intensive workloads.</p><p>The log buffer size defines the maximum amount of information that can be put in
a log buffer; if a log stripe unit is not set then buffer writes can be shorter
than the maximum, and hence there is no need to reduce the log buffer size for
fsync heavy workloads.</p><p>The default size of the log buffer is 32KB. The maximum size is 256KB and other
supported sizes are 64KB, 128KB or power of 2 multiples of the log stripe unit
between 32KB and 256KB. It can be configured by use of the <code class="literal">logbsize</code> mount
option.</p><p>The number of log buffers can also be configured to between 2 and 8. The default
is 8 log buffersi and can be configured by the use of the <code class="literal">logbufs</code> mount
option. It is rare that this needs to be configured, and it should only be
considered if there is limited memory and lots of XFS filesystems such that the
memory allocated to the log buffers would consume a significant amount of
memory. Reducing the number of log buffers tends to decrease log performance,
especially on log IO latency sensitive workloads, and so tuning this option is
typically not required.</p></div></div></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a id="Observation"></a>Part III. Observation</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="_todo"></a>Chapter 3. TODO</h2></div></div></div><p>TODO</p></div></div><div class="part"><div class="titlepage"><div><div><h1 class="title"><a id="Process"></a>Part IV. Tuning Processes</h1></div></div></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="_todo_2"></a>Chapter 4. TODO</h2></div></div></div><p>TODO</p></div></div></div></body></html>